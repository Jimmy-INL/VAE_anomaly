{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 0.4.1\n",
      "torchvision version: 0.2.1\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('torchvision version:', torchvision.__version__)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# general settings\n",
    "device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "# batchsize\n",
    "batchsize = 100\n",
    "\n",
    "# random seed\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random erasing transform\n",
    "class RandomErasing(object):\n",
    "    def __init__(self, erasing_prob, erasing_height = 4, erasing_width = 4):\n",
    "        self.erasing_prob = erasing_prob\n",
    "        self.erasing_height = erasing_height\n",
    "        self.erasing_width = erasing_width\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        if np.random.rand() < self.erasing_prob:\n",
    "            return self.random_erase(sample) # anomaly samples have negative label (use abs if you wanna see number)\n",
    "        else:\n",
    "            return sample\n",
    "            \n",
    "    def random_erase(self, tensor):\n",
    "        channel, height, width = tensor.size()\n",
    "        erasing_h = np.random.randint(height - self.erasing_height)\n",
    "        erasing_h = [erasing_h, erasing_h + self.erasing_height]\n",
    "        erasing_w = np.random.randint(width - self.erasing_width)\n",
    "        erasing_w = [erasing_w, erasing_w + self.erasing_width]\n",
    "        tensor[:, \\\n",
    "               erasing_h[0]:erasing_h[1], \\\n",
    "               erasing_w[0]:erasing_w[1]] = torch.ones(self.erasing_height, self.erasing_width)   \n",
    "        \n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ../../data/train_32x32.mat\n",
      "Using downloaded and verified file: ../../data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "# compose transforms (convert to PyTorch Tensor, Normalize, and random erasing(train:0.01,test:0.5 samples contaminated))\n",
    "tf_train = transforms.Compose([transforms.ToTensor(), \n",
    "                               RandomErasing(erasing_prob = 0.01),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# anomaly for test data will be added after\n",
    "tf_test = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# using SVHN dataset\n",
    "training_data = datasets.SVHN(root = '../../data', split = 'train', transform = tf_train, download = True)\n",
    "test_data = datasets.SVHN(root = '../../data', split = 'test', transform = tf_test, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training data: 58605\n",
      "The number of validation data 14652\n"
     ]
    }
   ],
   "source": [
    "# split to training data and validation data\n",
    "train_data, validation_data = train_test_split(training_data, test_size = 0.2, random_state = seed)\n",
    "print('The number of training data:', len(train_data))\n",
    "print('The number of validation data', len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data loader\n",
    "train_loader = DataLoader(train_data, batch_size = batchsize, shuffle = True)\n",
    "validation_loader = DataLoader(validation_data, batch_size = batchsize, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define downsampling parts for convenience\n",
    "class Downsampler(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize = 4, ssize = 2, psize = 1):\n",
    "        super(Downsampler, self).__init__()\n",
    "        self.cv = nn.Conv2d(in_channels, out_channels, kernel_size = ksize, stride = ssize, padding = psize)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.rl = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define upsampling parts for convenience\n",
    "class Upsampler(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize = 4, ssize = 2, psize = 1):\n",
    "        super(Upsampler, self).__init__()\n",
    "        self.tc = nn.ConvTranspose2d(in_channels, out_channels, kernel_size = ksize, stride = ssize, padding = psize)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.rl = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.tc(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.rl(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define VAE Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_z):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.cv1 = Downsampler(  3,  32) # out tensor size : (batchsize,  32, 16, 16)\n",
    "        self.cv2 = Downsampler( 32,  64) # out tensor size : (batchsize,  64,  8, 8)\n",
    "        self.cv3 = Downsampler( 64, 128) # out tensor size : (batchsize, 128,  4, 4)\n",
    "        self.cv4 = Downsampler(128, 256) # out tensor size : (batchsize, 256,  2, 2)\n",
    "        \n",
    "        self.fc5_mean   = nn.Linear(256*2*2, n_z)\n",
    "        self.fc5_logvar = nn.Linear(256*2*2, n_z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cv1(x)\n",
    "        out = self.cv2(out)\n",
    "        out = self.cv3(out)\n",
    "        out = self.cv4(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out_mean   = self.fc5_mean(out)\n",
    "        out_logvar = self.fc5_logvar(out)\n",
    "        \n",
    "        return out_mean, out_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define VAE Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_z):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_z, 256*2*2)\n",
    "        self.tc2 = Upsampler(256, 128)   # out tensor size : (batchsize, 256,  4,  4)\n",
    "        self.tc3 = Upsampler(128,  64)   # out tensor size : (batchsize, 128,  8,  8)\n",
    "        self.tc4 = Upsampler( 64,  32)   # out tensor size : (batchsize,  64, 16, 16)\n",
    "        self.tc5_mean    = nn.ConvTranspose2d(32, 3, kernel_size = 4, stride = 2, padding = 1) # (batchsize, 3, 32, 32)\n",
    "        self.tc5_logvar  = nn.ConvTranspose2d(32, 3, kernel_size = 4, stride = 2, padding = 1) # (batchsize, 3, 32, 32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        \n",
    "        out = out.view(out.size(0), 256, 2, 2)\n",
    "        \n",
    "        out = self.tc2(out)\n",
    "        out = self.tc3(out)\n",
    "        out = self.tc4(out)\n",
    "        out_mean = self.tc5_mean(out)\n",
    "        out_logvar  = self.tc5_logvar(out)\n",
    "        \n",
    "        return out_mean, out_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, n_z):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(n_z)\n",
    "        self.decoder = Decoder(n_z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_mean, embed_logvar = self.encoder(x)\n",
    "        \n",
    "        eps = torch.randn(embed_mean.size()).to(device)\n",
    "        z = (0.5 * embed_logvar).exp() * eps + embed_mean\n",
    "        \n",
    "        out_mean, out_logvar = self.decoder(z)\n",
    "        \n",
    "        return out_mean, out_logvar, embed_mean, embed_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters: 2921006\n",
      "\n",
      "Model:\n",
      " VAE(\n",
      "  (encoder): Encoder(\n",
      "    (cv1): Downsampler(\n",
      "      (cv): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU()\n",
      "    )\n",
      "    (cv2): Downsampler(\n",
      "      (cv): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU()\n",
      "    )\n",
      "    (cv3): Downsampler(\n",
      "      (cv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU()\n",
      "    )\n",
      "    (cv4): Downsampler(\n",
      "      (cv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU()\n",
      "    )\n",
      "    (fc5_mean): Linear(in_features=1024, out_features=500, bias=True)\n",
      "    (fc5_logvar): Linear(in_features=1024, out_features=500, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (fc1): Linear(in_features=500, out_features=1024, bias=True)\n",
      "    (tc2): Upsampler(\n",
      "      (tc): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU()\n",
      "    )\n",
      "    (tc3): Upsampler(\n",
      "      (tc): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU()\n",
      "    )\n",
      "    (tc4): Upsampler(\n",
      "      (tc): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (rl): ReLU()\n",
      "    )\n",
      "    (tc5_mean): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (tc5_logvar): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "\n",
      "Optimizer:\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0.0001\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# prepare network and optimizer\n",
    "n_z = 500\n",
    "net = VAE(n_z)\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001, weight_decay = 0.0001)\n",
    "                       \n",
    "# counting trainable parameters in model\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "                       \n",
    "# モデルの構造、オプティマイザの表示\n",
    "print('The number of trainable parameters:', num_trainable_params)\n",
    "print('\\nModel:\\n', net)\n",
    "print('\\nOptimizer:\\n', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss functions\n",
    "def loss_D(embed_mean, embed_logvar):\n",
    "    loss_d = 0.5 * torch.sum(-embed_logvar - 1 + embed_logvar.exp() + embed_mean.pow(2))\n",
    "    return loss_d\n",
    "\n",
    "def loss_A(out_logvar):\n",
    "    log2pi = float(np.log(2*np.pi))\n",
    "    loss_a = 0.5 * torch.sum(log2pi + out_logvar)\n",
    "    return loss_a\n",
    "    \n",
    "def loss_M(out_mean, out_logvar, in_x):\n",
    "    loss_m = 0.5 * torch.sum( (out_mean - in_x).pow(2) / out_logvar.exp() )\n",
    "    return loss_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function run training for 1 epoch\n",
    "def train(train_loader):\n",
    "    net.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    for inputs, _ in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        out_mean, out_logvar, embed_mean, embed_logvar = net(inputs)\n",
    "        loss = loss_D(embed_mean, embed_logvar) + loss_A(out_logvar) + loss_M(out_mean, out_logvar, inputs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    average_loss = loss / len(train_loader.dataset)\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(validation_loader):\n",
    "    net.eval()\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in validation_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            out_mean, out_logvar, embed_mean, embed_logvar = net(inputs)\n",
    "            loss = loss_D(embed_mean, embed_logvar) + loss_A(out_logvar) + loss_M(out_mean, out_logvar, inputs)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    average_loss = loss / len(validation_loader.dataset)\n",
    "    \n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/30] train_loss:-0.1091 validation_loss:-4.6372\n",
      "epoch[2/30] train_loss:0.4647 validation_loss:-5.2630\n",
      "epoch[3/30] train_loss:0.3593 validation_loss:-8.2043\n",
      "epoch[4/30] train_loss:-0.0276 validation_loss:-8.4736\n",
      "epoch[5/30] train_loss:-0.0355 validation_loss:-10.6588\n",
      "epoch[6/30] train_loss:0.3148 validation_loss:-10.3153\n",
      "epoch[7/30] train_loss:0.7152 validation_loss:-11.0284\n",
      "epoch[8/30] train_loss:0.7460 validation_loss:-12.2136\n",
      "epoch[9/30] train_loss:-0.1768 validation_loss:-12.8444\n",
      "epoch[10/30] train_loss:2.0114 validation_loss:-12.3208\n",
      "epoch[11/30] train_loss:0.2368 validation_loss:-13.4992\n",
      "epoch[12/30] train_loss:0.4864 validation_loss:-13.0263\n",
      "epoch[13/30] train_loss:0.1297 validation_loss:-12.6830\n",
      "epoch[14/30] train_loss:0.3276 validation_loss:-14.0903\n",
      "epoch[15/30] train_loss:0.6828 validation_loss:-13.7336\n",
      "epoch[16/30] train_loss:0.1891 validation_loss:-14.0375\n",
      "epoch[17/30] train_loss:-0.2484 validation_loss:-14.4665\n",
      "epoch[18/30] train_loss:0.3398 validation_loss:-14.8557\n",
      "epoch[19/30] train_loss:-0.0764 validation_loss:-14.9441\n",
      "epoch[20/30] train_loss:0.5691 validation_loss:-14.7813\n",
      "epoch[21/30] train_loss:-0.0553 validation_loss:-15.1076\n",
      "epoch[22/30] train_loss:-0.1267 validation_loss:-15.2229\n",
      "epoch[23/30] train_loss:0.9755 validation_loss:-15.5194\n",
      "epoch[24/30] train_loss:0.2027 validation_loss:-14.8374\n",
      "epoch[25/30] train_loss:-0.2276 validation_loss:-15.9532\n",
      "epoch[26/30] train_loss:0.8938 validation_loss:-15.1406\n",
      "epoch[27/30] train_loss:2.5980 validation_loss:-15.6926\n",
      "epoch[28/30] train_loss:0.3550 validation_loss:-16.1335\n",
      "epoch[29/30] train_loss:0.6837 validation_loss:-15.7362\n"
     ]
    }
   ],
   "source": [
    "# run training and save trained model\n",
    "output_dir = '../../data/VAE_anomaly/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "n_epochs = 30\n",
    "train_loss_list = []\n",
    "validation_loss_list = []\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_loader)\n",
    "    validation_loss = validation(validation_loader)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    validation_loss_list.append(validation_loss)\n",
    "    \n",
    "    print('epoch[%d/%d] train_loss:%1.4f validation_loss:%1.4f' %(epoch+1, n_epochs, train_loss, validation_loss))\n",
    "    \n",
    "np.save(output_dir + 'train_loss_list.npy', np.array(train_loss_list))\n",
    "np.save(output_dir + 'validation_loss_list.npy', np.array(validation_loss_list))\n",
    "\n",
    "torch.save(net.state_dict(), output_dir + 'VAE_toy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
